defaults:
  - reward_model@reward_model: dp_reward_model
  - rollout@actor_rollout_ref.rollout: rollout
  - model@actor_rollout_ref.model: hf_model
  - critic@critic: dp_critic
  - _self_

trainer:
  nnodes: 1
  n_gpus_per_node: 8
  device: cuda
  project_name: verl_generation
  experiment_name: default_generation

data:
  path: ~/data/rlhf/math/test.parquet
  prompt_key: prompt
  n_samples: 5
  output_path: /opt/tiger/math_Qwen2-7B-Instruct.parquet
  # Run 8 prompts, each with 5 samples in each batch by default
  batch_size: 8
  # Optional dataset splitting (match reference script semantics)
  total_splits: 1   # >=1; when >1, add _split{current}_of_{total} to filenames
  current_split: 0   # 0-indexed; must be < total_splits
  # This is ignored (for compatibility with training script CLI)
  return_raw_chat: True

reward_model:
  config:
    version: "v2"

    require_think_end: False
    strip_comma_from_answer: False

    acceleration_ratio_reward: 1.0
    acceleration_ratio_reward_factor: 1.0
    acceleration_ratio_clip_max: 0.1
    acceleration_ratio_curve: linear

    verbose: 1

ray_kwargs:
  ray_init:
    num_cpus: null # `None` means using all CPUs, which might cause hang if limited in systems like SLURM. Please set to a number allowed then.
  timeline_json_file: null
