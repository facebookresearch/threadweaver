"""
This module defines data structures and base classes for reward calculations
to evaluate model responses for various problem types, including math and coding.
"""

from typing import Optional
from dataclasses import dataclass, field
from enum import Enum

@dataclass
class RewardConfig:
    version: Optional[str] = None

    # Use LLM as ORM to evaluate correctness.
    use_math_orm: bool = False

    # General reward constants.
    correct_reward: float = 1.0
    incorrect_reward: float = -1.0
    format_error_reward: float = -1.0
    unk_error_reward: float = -1.0

    # If enabled, the reward will be -require_end_think_reward if the trajectory does not end with </think> (or </Think>)
    require_think_end: bool = True
    # If enabled, comma will be stripped from the answer before checking correctness. This is sometimes needed because `_strip_properly_formatted_commas` does not always remove commas in the answer.
    strip_comma_from_answer: bool = False

    # The factor is controlled by algorithm.second_reward_coef
    # none: always 0
    # see `math_rewardv2.py` for other options
    second_reward_type: str = "none"

    # Encourage using <Parallel> in trajectory
    # This is added to the reward if the answer if correct
    parallel_reward: float = 0.
    # If enabled, the parallel count will be clipped to this value before the reward calculation
    parallel_clip_max: int = 100000

    # Only used in version "v2": if there is parallel, add this reward
    parallel_rewardv2: float = 0.

    # If parallel is not working, parallel ratio reward is -parallel_ratio_reward
    # If parallel is working, parallel ratio reward is parallel_ratio_reward * ratio * parallel_ratio_reward_factor
    parallel_ratio_reward: float = 0.
    parallel_ratio_reward_factor: float = 1.0
    parallel_ratio_clip_max: float = 1.0
    # type: linear/cosine/tanh
    parallel_ratio_curve: str = "linear"

    acceleration_ratio_reward: float = 0.
    acceleration_ratio_reward_factor: float = 1.0
    acceleration_ratio_clip_max: float = 1.0
    # type: linear/cosine/tanh
    acceleration_ratio_curve: str = "linear"

    # If enabled, the reward will be -parallel_format_error_reward or -parallel_format_error_v2_reward if the trajectory is ill-formatted
    parallel_format_error_reward_enabled: bool = False
    parallel_format_error_reward: float = -1.0
    parallel_format_error_v2_reward_enabled: bool = False
    parallel_format_error_v2_reward: float = -1.0
    # Allow content other than whitespace between </Path> and the next <Path> in one <Parallel> block
    parallel_format_error_v2_allow_nonempty_whitespace: bool = False
    parallel_format_error_v2_skip_conclusion_check: bool = False

    # If enabled, the reward will be -parallel_absent_penalty if there is no parallel.
    # None: True if `parallel_format_error_reward_enabled`. `False` otherwise. This is for compatibility.
    treat_no_parallel_as_format_error: Optional[bool] = None

    # If enabled, automatically handle immediate stop tokens (</think> or </Think>) in <Path> in the <Parallel> blocks.
    allow_immediate_stop: bool = False

    verbose: Optional[int] = None

class RewardType(Enum):
    """
    Enum class representing the different types of rewards that can be assigned.

    Attributes:
        MATH (str): Represents a math-related problem type.
        CODE (str): Represents a coding-related problem type.
        UNK (str): Represents an unknown or unclassified problem type.
    """
    MATH = 'MATH'
    CODE = 'CODE'
    UNK = 'UNK'


@dataclass
class RewardInput:
    """Data structure for input required to calculate rewards.

    Attributes:
        problem (str): The original problem text or prompt provided to the model.
        model_response (str): The response generated by the model that needs evaluation.
        problem_type (RewardType): The category of the problem (e.g., math, code) to be evaluated.
        ground_truth (dict): Additional contextual information necessary for evaluation:
            - For math problems: This may include the ground truth answer.
            - For coding problems: This may include unit tests to validate the solution.
    """
    problem: str
    model_response: str
    model_response_token_ids: Optional[list] = None
    problem_type: RewardType = RewardType.UNK
    ground_truth: dict = field(default_factory=dict)


@dataclass
class RewardOutput:
    """Data structure for the output of reward calculations.

    Attributes:
        reward (float): The computed reward value based on the evaluation of the model's response.
        is_correct (bool): A boolean flag indicating whether the model's response is deemed correct.
    """
    reward: float
    second_reward: float
    is_correct: bool
    extra_info: dict


class RewardFn:
    """Abstract base class for defining reward calculation strategies.

    This class should be subclassed to implement specific reward calculation logic.
    The __call__ method must be overridden to provide the functionality for evaluating
    the input and returning the corresponding reward output.
    """
    def __init__(self, config: RewardConfig):
        self.config = config

    def __call__(self, input: RewardInput) -> RewardOutput:
        raise NotImplementedError("Subclasses must implement this method.")
